# [Data-Snooping, Technical Trading Rule Performance, and the Bootstrap](https://www.kevinsheppard.com/images/0/0d/Sullivan_Timmermann_White.pdf)
## By R. Sullivan, A. Timmermann, and H. White.
### A Review

  Up until the early 2000s, some data-mining practices were extremely difficult to quantify. Of particular difficulty was quantifying data-snooping, which refers to the reuse of a dataset in order to select a good trading rule from a universe of other similar rules. Therefore, the majority of lucrative rules to date could have been simply due to chance rather than to market insights. In the paper Data-Snooping, Technical Trading Rule Performance, and the Bootstrap, Sullivan et. al. (1999) present a novel methodology to quantify data-snooping and evaluate the merit of a trading rule in terms of both mean returns and Sharpe ratio. They do this by implementing a bootstrapped evaluation that takes into account dependencies between results for different rules. The paper applies this methodology to a densely cited study by Brock, Lakonishok, and LeBaron (BLL), whose set of 26 technical trading rules, including several trading range breaks, significantly outperform a buy-and-hold strategy on 90 years worth of daily stock prices from the DJX. Sullivan et. al., demonstrate that, according to the bootstrapped evaluation and measured dependencies between trading rules, BLL’s strategy present a case of data-snooping. Additionally, the authors show that the best trading rules underperformed on out-of-sample evaluations. The approach developed in the aforementioned paper should be a part of every investor’s tool kit, as it allows an investor to select the best performing strategy among a series of multiple strategies, having a certain degree of confidence that the results are not simply due to good luck.<br>
  
  To identify data-snooping, Sullivan et. al. carefully designed a universe of 8,000 trading rules similar to BLL’s with varying parameterizations [1], and applied a bootstrap procedure inspired by Politis and Romano’s stationary bootstrap (1994) and White’s (2000) Reality Check. The procedure evaluates the distribution of a suitable performance measure giving consideration to the full set of models that lead to the best performing rule. It does so by quantifying the nominal p-value, derived from the null-hypothesis that the best trading rule does not beat the benchmark, as well as White’s Reality Check p-value, derived from the same null-hypothesis but with a full universe of trading rules. The difference between these two p-values is interpreted as the magnitude of the data-snooping bias, from which one can determine whether the best trading rule actually carries valuable economic information or if it is simply a factor of it being selected from a large set of rules.<br>
  
  In addition to imperative out-of-sample evaluations, implementation of White’s Reality Check should be undertaken in order to boost confidence that a signal is worthy when it is being selected from a large number of strategies. The authors also cite studies that show the method’s usability in any traded instruments, including stocks, exchanges and interest rates, and other identification of ‘anomalies’ in cross- sectional tests of asset pricing models. Varying time-dependent properties, though, should be noted when deciding whether to use White’s Reality Check, as the stationary bootstrap method requires for the dataset to be stationary and weakly dependent. By conducting a 10 year out-of-sample evaluation, the authors show that the accuracy of the Reality Check does not falter with varying holding periods on the DJIX, granted one takes into account significant anomalies such as October 19, 1987 or improvements in market efficiency. When put in practice, this methodology only holds as long as the universe is produced with extensive care. Effort must be allocated to both keep track of all rules tested, and being able to produce trading rules that span a similar space as the rule being evaluated. Failure in doing so has been shown to cause overestimation in data mining bias, in particular when strategies in the universe have negative expectancy or are entirely irrelevant in terms of similarities to each other (Reinhard, 2001). 


[1] Rules selected satisfy two characteristics: that they were widely known in a substantial part of the sample period in order to eliminate ex ante risk premia, and that they span a larger space than the original 26 BLL rules so as to account for snooping within the space spanned by the included rules.

# References
1. [Brock, William, Josef Lakonishok, and Blake LeBaron. “Simple Technical Trading Rules and the Stochastic Properties of Stock Returns.” Journal of Finance 47, pp.1731-1764, December 1992.](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1992.tb04681.x)
2. [Politis, Dimitris and Joseph P. Romano. “The Stationary Bootstrap.” Journal of the American Statistical Association, vol. 89, 1994. pp 1303-1313, April, 1992.](https://statistics.stanford.edu/sites/default/files/EFS%20NSF%20365.pdf)
3. [Qi, Min, and Yangru Wu. “Technical Trading-Rule Profitability, Data Snooping, and Reality Check: Evidence form the Foreign Exchange Market.” Journal of Money, Credit and Banking, vol. 38, no.8, pp. 2135-2158, December, 2006](http://andromeda.rutgers.edu/~yangruwu/TechTrade_JMCB.pdf)
4. [Reinhard Hansen, Peter. “A Test for Superior Predictive Ability.” Journal of Business & Economic Statistics, vol. 23, no. 4, pp. 365-380, October, 2005.](http://www-siepr.stanford.edu/workp/swp05003.pdf)
5. [White, Halbert. “A Reality Check for Data Snooping.” Econometrica, Vol. 65, no. 5, 2000 pp. 1097–1126, September 2000.](https://www.ssc.wisc.edu/~bhansen/718/White2000.pdf)

# Code Reference Articles
1. [Cristian Dima. "White's Reality Check for Data Snooping in R"](http://www.cristiandima.com/white-s-reality-check-for-data-snooping-in-r/)
2. [The Financial Hacker. _A New View on Algorithmic Trading_. "White's Reality Check"](http://www.financial-hacker.com/whites-reality-check/)
